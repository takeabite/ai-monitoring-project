# app.py
import streamlit as st
import pandas as pd, time, os, json
from config import LOG_FILE
from datetime import datetime
from streamlit_autorefresh import st_autorefresh
from collections import Counter, defaultdict
import plotly.express as px

st.set_page_config(page_title="Real-time TX Monitor", layout="wide")
st.title("ğŸ“ˆ AI ê¸°ë°˜ ì‹¤ì‹œê°„ ê±°ë˜ ëª¨ë‹ˆí„°ë§")

# auto refresh every 3 seconds
count = st_autorefresh(interval=3000, limit=None, key="autorefresh")

st.sidebar.header("Settings")
n_recent = st.sidebar.slider("ìµœê·¼í‘œì‹œê±´ìˆ˜", 50, 1000, 200)

ANOMALY_FILE = LOG_FILE + ".anomalies.jsonl"

def read_log(limit_lines=5000):
    if not os.path.exists(LOG_FILE):
        return pd.DataFrame()
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()[-limit_lines:]
    import re
    pattern = re.compile(
        r"\[(?P<ts>[^\]]+)\]\s+status=(?P<status>\w+)\s+latency=(?P<lat>[\d\.]+)ms\s+merchant=(?P<merchant>\S+)\s+region=(?P<region>\S+)\s+amount=(?P<amount>[\d\.]+)"
    )
    rows=[]
    for L in lines:
        m = pattern.search(L)
        if m:
            d = m.groupdict()
            rows.append({
                "timestamp": pd.to_datetime(d["ts"]),
                "status": 1 if d["status"]=="SUCCESS" else 0,
                "latency": float(d["lat"]),
                "merchant": d["merchant"],
                "region": d["region"],
                "amount": float(d["amount"]),
                "raw": L.strip()
            })
    if not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    df = df.sort_values("timestamp").tail(n_recent)
    return df

def read_anomalies(limit=1000):
    items = []
    if not os.path.exists(ANOMALY_FILE):
        return pd.DataFrame()
    with open(ANOMALY_FILE, "r", encoding="utf-8") as f:
        for l in f.readlines()[-limit:]:
            try:
                items.append(json.loads(l))
            except:
                continue
    if not items:
        return pd.DataFrame()
    df = pd.DataFrame(items)
    # normalize datetime
    if "detected_at" in df.columns:
        df["detected_at"] = pd.to_datetime(df["detected_at"])
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"])
    return df

df = read_log()
an_df = read_anomalies(1000)

col1, col2, col3 = st.columns([1,1,1])
with col1:
    st.metric("ì´ ê±°ë˜(í‘œì‹œ)", len(df))
with col2:
    if not df.empty:
        st.metric("ìµœê·¼ í‰ê·  ì§€ì—°(ms)", f"{df['latency'].mean():.1f}")
    else:
        st.metric("ìµœê·¼ í‰ê·  ì§€ì—°(ms)", "-")
with col3:
    if not df.empty:
        reject_rate = 100*(1 - df['status'].mean())
        st.metric("ìµœê·¼ ê±°ì ˆìœ¨(%)", f"{reject_rate:.2f}")
    else:
        st.metric("ìµœê·¼ ê±°ì ˆìœ¨(%)", "-")

# --- ì´ìƒ íƒ€ì… í•œê¸€ ë§¤í•‘ ---
TYPE_KO = {
    "autoencoder": "AI ì´ìƒ(AutoEncoder)",
    "high_latency": "ê³ ì§€ì—°",
    "high_amount": "ê³ ì•¡",
    "unknown_merchant": "ë¯¸ë“±ë¡ ìƒì ",
    "unknown_region": "ë¯¸ë“±ë¡ ì§€ì—­",
    "failure": "ê±°ë˜ ì‹¤íŒ¨",
    "off_hour": "ì‹¬ì•¼ ê±°ë˜",
    "burst": "ë²„ìŠ¤íŠ¸",
    "card_testing": "ì¹´ë“œ í…ŒìŠ¤íŠ¸(ì†Œì•¡ ë°˜ë³µ)",
    "merchant_spike": "ìƒì  í­ì£¼",
    "composite": "ë³µí•© ì´ìƒ"
}

def types_to_ko_list(types):
    if isinstance(types, list):
        return [TYPE_KO.get(t, t) for t in types]
    # try to parse string representation like "['a','b']"
    try:
        parsed = eval(types)
        if isinstance(parsed, list):
            return [TYPE_KO.get(t, t) for t in parsed]
    except:
        pass
    return []

# anomaly summary
st.subheader("ì´ìƒ ì´ë²¤íŠ¸ ìš”ì•½")
if an_df.empty:
    st.write("ê°ì§€ëœ ì´ìƒ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    # add a column with Korean labels for display (keeps existing column for table)
    an_df["types_ko"] = an_df["types"].apply(types_to_ko_list)

    # í•„í„°ë§: UIì—ì„œ ì œì™¸í•  ì´ìƒ íƒ€ì… (ì›ë¬¸ í‚¤)
    SKIP_TYPES = {"merchant_spike", "off_hour", "unknown_region"}

    # flatten and count using ì›ë¬¸ keys -> í•œê¸€ ë¼ë²¨ ë§¤í•‘, ì œì™¸ íƒ€ì…ëŠ” ê±´ë„ˆëœ€
    type_counts = Counter()
    for raw in an_df["types"].tolist():
        keys = []
        if raw is None:
            continue
        if isinstance(raw, list):
            keys = raw
        elif isinstance(raw, str):
            # JSONìœ¼ë¡œ ì €ì¥ëœ ê²½ìš° ìš°ì„  íŒŒì‹±, ì‹¤íŒ¨í•˜ë©´ evalë¡œ ì‹œë„
            try:
                parsed = json.loads(raw)
                if isinstance(parsed, list):
                    keys = parsed
            except Exception:
                try:
                    parsed = eval(raw)
                    if isinstance(parsed, list):
                        keys = parsed
                except Exception:
                    keys = []
        else:
            keys = []
        for k in keys:
            if k in SKIP_TYPES:
                continue
            type_counts[ TYPE_KO.get(k, k) ] += 1
    tc_df = pd.DataFrame(type_counts.items(), columns=["type","count"]).sort_values("count", ascending=False)

    # horizontal bar: ê¸€ì í¬ê¸° í‚¤ì›€
    if not tc_df.empty:
        fig = px.bar(tc_df, x="type", y="count", text="count")
        fig.update_traces(textposition="outside", textfont=dict(size=12))
        fig.update_layout(
            font=dict(size=16), 
            margin=dict(l=20, r=20, t=30, b=40),
            height=420,
            bargap=0.2
        )
        fig.update_xaxes(tickfont=dict(size=14), title_text="")
        fig.update_yaxes(tickfont=dict(size=14), title_text="ê±´ìˆ˜")

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("ì´ìƒ ìœ í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # time series: anomalies per minute
    ts = an_df.copy()
    if "detected_at" in ts.columns and not ts["detected_at"].isna().all():
        ts["minute"] = ts["detected_at"].dt.floor("min")
        per_min = ts.groupby("minute").size().rename("count").reset_index()
        if not per_min.empty:
            st.line_chart(per_min.set_index("minute")["count"])

    st.subheader("ìµœê·¼ ì´ìƒ ëª©ë¡ (ìµœì‹  100)")
    disp = an_df.sort_values("detected_at", ascending=False).head(100)
    if not disp.empty:
        # ë³´ì—¬ì¤„ ë•Œ í•œêµ­ì–´ íƒ€ì… ì»¬ëŸ¼ë„ í•¨ê»˜ í‘œì‹œ
        disp_display = disp[["detected_at","types","types_ko","merchant","region","amount","latency","err","raw"]].reset_index(drop=True)
        st.dataframe(disp_display)

st.subheader("Latency ì¶”ì„¸ (ìµœê·¼ ê±°ë˜)")
if not df.empty:
    st.line_chart(df.set_index("timestamp")["latency"])

st.subheader("ê±°ë˜ ëª©ë¡ (ìµœê·¼)")
if not df.empty:
    st.dataframe(df.sort_values("timestamp", ascending=False).reset_index(drop=True))

st.warning("AI íƒì§€(í…”ë ˆê·¸ë¨ ì „ì†¡)ëŠ” ai_monitor.pyê°€ ìˆ˜í–‰í•©ë‹ˆë‹¤. anomalies.jsonl íŒŒì¼ì„ í†µí•´ UIì— ì´ìƒ ë‚´ì—­ì´ ì§‘ê³„ë©ë‹ˆë‹¤.")
